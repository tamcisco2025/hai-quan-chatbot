# Chatbot Tra cá»©u Nghiá»‡p vá»¥ Háº£i quan (RAG)

Bá»™ khung mÃ£ nguá»“n má»Ÿ, miá»…n phÃ­, cháº¡y trÃªn **Hugging Face Spaces** vá»›i **Gradio + LlamaIndex + Chroma**. Tráº£ lá»i **tiáº¿ng Viá»‡t**, **ngáº¯n gá»n cÃ³ trÃ­ch dáº«n Ä‘iá»u/khoáº£n**, chá»‰ dá»±a trÃªn **vÄƒn báº£n phÃ¡p luáº­t VN (Luáº­t/Nghá»‹ Ä‘á»‹nh/ThÃ´ng tÆ°)** do báº¡n táº£i lÃªn. Há»— trá»£ **táº£i file trá»±c tiáº¿p trÃªn UI** vÃ  **lá»c theo loáº¡i vÄƒn báº£n / nÄƒm / Ä‘iá»u-khoáº£n**.

---

## 1) HÆ°á»›ng dáº«n triá»ƒn khai nhanh (5â€“10 phÃºt)

1. **Táº¡o GitHub repo** (Public) vÃ  tháº£ 3 file sau vÃ o root:

   * `app.py`
   * `requirements.txt`
   * `README.md` (báº¡n cÃ³ thá»ƒ dÃ¹ng chÃ­nh file nÃ y)
   * Táº¡o thÆ° má»¥c `data/` (Ä‘á»ƒ cÃ¡c vÄƒn báº£n Ä‘áº§u tiÃªn vÃ o Ä‘Ã¢y, <50 file PDF/DOCX)

2. **Táº¡o Hugging Face Space**

   * ÄÄƒng nháº­p ğŸ‘‰ New Space â†’ chá»n **Gradio** â†’ Public.
   * Káº¿t ná»‘i vá»›i repo GitHub hoáº·c **Upload** trá»±c tiáº¿p `app.py`, `requirements.txt`, `README.md`, thÆ° má»¥c `data/`.
   * Chá» build xong lÃ  cÃ³ URL cÃ´ng khai.

3. **DÃ¹ng thá»­**

   * GÃµ cÃ¢u há»i (VD: *"Äiá»u kiá»‡n miá»…n kiá»ƒm tra thá»±c táº¿ hÃ ng hÃ³a theo ThÃ´ng tÆ° 39/2018?"*).
   * Thá»­ **Upload** thÃªm file ngay trÃªn giao diá»‡n, chá»n bá»™ lá»c "Loáº¡i vÄƒn báº£n/NÄƒm/Äiá»u" rá»“i há»i láº¡i.

> Miá»…n phÃ­ hoÃ n toÃ n: dÃ¹ng CPU cá»§a HF Spaces. Tá»‘c Ä‘á»™ vá»«a Ä‘á»§ cho demo/cá»™ng Ä‘á»“ng (<50 file).

---

## 2) NguyÃªn táº¯c tráº£ lá»i & thÃ´ng Ä‘iá»‡p báº¯t buá»™c

* **Chá»‰** dá»±a trÃªn tÃ i liá»‡u Ä‘Ã£ táº£i (RAG). Náº¿u khÃ´ng cÃ³ cÄƒn cá»© phÃ¹ há»£p:
  **"Chá»§ Ä‘á» nÃ y tÃ´i chÆ°a Ä‘Æ°á»£c cáº­p nháº­t. Xin hÃ£y chá»n chá»§ Ä‘á» khÃ¡c."**
* Tráº£ lá»i **ngáº¯n gá»n**, cÃ³ **trÃ­ch dáº«n**: `[TÃªn vÄƒn báº£n â€“ Äiá»u/Khoáº£n (náº¿u cÃ³) â€“ Trang]`.

---

## 3) `requirements.txt`

```txt
# UI
gradio==4.44.0

# RAG
llama-index==0.11.0
llama-index-embeddings-huggingface
llama-index-vector-stores-chroma
chromadb==0.5.5
sentence-transformers>=2.7.0

# Models (CPU)
transformers>=4.44.0
# DÃ²ng dÆ°á»›i giÃºp cÃ i torch CPU nhanh hÆ¡n trÃªn nhiá»u mÃ´i trÆ°á»ng, cÃ³ thá»ƒ bá» náº¿u build lá»—i
torch --extra-index-url https://download.pytorch.org/whl/cpu

# Parsers
pypdf
python-docx
unidecode
```

> **Ghi chÃº**: Náº¿u `torch` lá»—i trÃªn HF Spaces, xÃ³a dÃ²ng `torch ...` Ä‘á»ƒ HF tá»± chá»n phiÃªn báº£n phÃ¹ há»£p.

---

## 4) `app.py`

```python
import os
import re
import io
import time
from typing import List, Dict, Any

import gradio as gr
from unidecode import unidecode

# LlamaIndex core
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    Settings,
    Document,
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# Vector store: Chroma
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb

# Lightweight open-source LLM for CPU (tiáº¿ng Viá»‡t khÃ¡ á»•n vá»›i prompt ngáº¯n)
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# =========================
# 0) Cáº¥u hÃ¬nh
# =========================
DATA_DIR = os.getenv("DATA_DIR", "data")
UPLOAD_DIR = os.path.join(DATA_DIR, "uploads")
INDEX_NAME = "hai-quan-vn"

# Model ngáº¯n gá»n, cháº¡y CPU nhanh
LLM_NAME = os.getenv("LLM_NAME", "Qwen/Qwen2.5-1.5B-Instruct")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2")

FALLBACK_MSG = "Chá»§ Ä‘á» nÃ y tÃ´i chÆ°a Ä‘Æ°á»£c cáº­p nháº­t. Xin hÃ£y chá»n chá»§ Ä‘á» khÃ¡c."

# NgÆ°á»¡ng tá»‘i thiá»ƒu coi lÃ  cÃ³ cÄƒn cá»©
SIM_THRESHOLD = float(os.getenv("SIM_THRESHOLD", "0.30"))
TOP_K = int(os.getenv("TOP_K", "5"))
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "1000"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "150"))

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(UPLOAD_DIR, exist_ok=True)

# =========================
# 1) LLM & Embeddings
# =========================
print("\n[Init] Loading LLM ...")
tok = AutoTokenizer.from_pretrained(LLM_NAME)
llm_pipe = pipeline(
    task="text-generation",
    model=AutoModelForCausalLM.from_pretrained(LLM_NAME),
    tokenizer=tok,
    max_new_tokens=512,
    do_sample=False,
)

Settings.embed_model = HuggingFaceEmbedding(model_name=EMBEDDING_MODEL)
Settings.node_parser = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)

# =========================
# 2) Tiá»‡n Ã­ch metadata & parser
# =========================
TYPE_PATTERNS = [
    (r"\blu[aÃ¢]t\b", "Luáº­t"),
    (r"ngh[iá»‹]\s*Ä‘[iá»‹]nh|nd\b", "Nghá»‹ Ä‘á»‹nh"),
    (r"th[oÃ´]ng\s*t[uÆ°]|tt\b", "ThÃ´ng tÆ°"),
]

YEAR_RE = re.compile(r"(20\d{2}|19\d{2})")
DIEU_RE = re.compile(r"\bÄiá»u\s+(\d+[A-Z]?)", re.IGNORECASE)
KHOAN_RE = re.compile(r"\bKhoáº£n\s+(\d+[A-Z]?)", re.IGNORECASE)


def guess_type_and_year(name: str) -> Dict[str, Any]:
    lower = unidecode(name.lower())
    vtype = None
    for pat, label in TYPE_PATTERNS:
        if re.search(pat, lower):
            vtype = label
            break
    year_match = YEAR_RE.search(name)
    year = int(year_match.group(1)) if year_match else None
    return {"loai": vtype, "nam": year}


def extract_refs(text: str) -> Dict[str, str]:
    d = DIEU_RE.search(text)
    k = KHOAN_RE.search(text)
    return {"dieu": d.group(1) if d else None, "khoan": k.group(1) if k else None}

# =========================
# 3) Dá»±ng / náº¡p Index
# =========================
print("[Init] Building Chroma store ...")
chroma_client = chromadb.Client()
collection = chroma_client.get_or_create_collection(INDEX_NAME)
vector_store = ChromaVectorStore(chroma_collection=collection)
index: VectorStoreIndex = None


def has_supported_files(path: str) -> bool:
    """Kiá»ƒm tra thÆ° má»¥c cÃ³ chá»©a Ã­t nháº¥t 1 file .pdf/.docx/.txt hay khÃ´ng."""
    if not os.path.isdir(path):
        return False
    for root, _, files in os.walk(path):
        if any(f.lower().endswith((".pdf", ".docx", ".txt")) for f in files):
            return True
    return False


def load_documents_from_dir(path: str) -> List[Document]:
    # Bá» qua náº¿u thÆ° má»¥c trá»‘ng/khÃ´ng cÃ³ file há»— trá»£ Ä‘á»ƒ trÃ¡nh ValueError
    if not has_supported_files(path):
        return []
    reader = SimpleDirectoryReader(
        input_dir=path,
        required_exts=[".pdf", ".docx", ".txt"],
        recursive=True,
    )
    docs = reader.load_data()
    enriched = []
    for d in docs:
        meta = d.metadata or {}
        name = meta.get("file_name") or os.path.basename(meta.get("filepath", ""))
        ty = guess_type_and_year(name)
        meta.update(ty)
        # Giá»¯ nguyÃªn cÃ¡c metadata cÃ³ sáºµn nhÆ° page_label náº¿u cÃ³
        enriched.append(Document(text=d.text, metadata=meta))
    return enriched


def build_or_update_index(base_dirs: List[str]):
    global index
    docs: List[Document] = []
    for p in base_dirs:
        docs.extend(load_documents_from_dir(p))
    if not docs:
        # táº¡o index rá»—ng Ä‘á»ƒ khÃ´ng lá»—i
        index = VectorStoreIndex.from_documents([], vector_store=vector_store)
        return
    index = VectorStoreIndex.from_documents(docs, vector_store=vector_store)


print("[Init] Loading documents & indexing ...")
dirs = [p for p in [DATA_DIR, UPLOAD_DIR] if has_supported_files(p)]
if not dirs:
    # táº¡o index rá»—ng Ä‘á»ƒ app váº«n cháº¡y khi chÆ°a cÃ³ tÃ i liá»‡u
    index = VectorStoreIndex.from_documents([], vector_store=vector_store)
else:
    build_or_update_index(dirs)
query_engine = index.as_query_engine(similarity_top_k=TOP_K, response_mode="compact")

# =========================
# 4) Truy váº¥n cÃ³ bá»™ lá»c
# =========================
SYSTEM_PROMPT = (
    "Báº¡n lÃ  trá»£ lÃ½ nghiá»‡p vá»¥ háº£i quan Viá»‡t Nam. Chá»‰ tráº£ lá»i dá»±a trÃªn tÃ i liá»‡u Ä‘Ã£ cung cáº¥p. "
    "Náº¿u khÃ´ng cÃ³ cÄƒn cá»© phÃ¹ há»£p, hÃ£y tráº£ lá»i Ä‘Ãºng thÃ´ng Ä‘iá»‡p: 'Chá»§ Ä‘á» nÃ y tÃ´i chÆ°a Ä‘Æ°á»£c cáº­p nháº­t. Xin hÃ£y chá»n chá»§ Ä‘á» khÃ¡c.' "
    "Tráº£ lá»i ngáº¯n gá»n, rÃµ rÃ ng, kÃ¨m trÃ­ch dáº«n nguá»“n theo dáº¡ng [TÃªn vÄƒn báº£n â€“ Äiá»u/Khoáº£n â€“ Trang]."
)


def filter_nodes_by_metadata(nodes, loai: str = None, nam: int = None, dieu: str = None, khoan: str = None):
    # Lá»c thÃ´ sau khi retrieve: giá»¯ nhá»¯ng node cÃ³ metadata phÃ¹ há»£p
    def ok(node):
        m = node.metadata or {}
        if loai and m.get("loai") != loai:
            return False
        if nam and m.get("nam") != nam:
            return False
        # Náº¿u ngÆ°á»i dÃ¹ng yÃªu cáº§u Äiá»u/Khoáº£n, Æ°u tiÃªn Ä‘oáº¡n cÃ³ chá»©a
        t = node.get_content(metadata_mode="none")
        if dieu and (f"Äiá»u {dieu}" not in t and f"Äiá»€u {dieu}" not in t):
            return False
        if khoan and (f"Khoáº£n {khoan}" not in t and f"Khoáº¢n {khoan}" not in t):
            return False
        return True

    kept = [n for n in nodes if ok(n)]
    return kept if kept else nodes  # náº¿u lá»c rá»—ng, tráº£ vá» danh sÃ¡ch gá»‘c Ä‘á»ƒ khÃ´ng máº¥t thÃ´ng tin


def format_citations(source_nodes) -> str:
    cites = []
    for sn in source_nodes:
        meta = sn.metadata or {}
        name = meta.get("file_name") or os.path.basename(meta.get("filepath", "Nguon"))
        page = meta.get("page_label") or meta.get("page")
        # cá»‘ gáº¯ng Ä‘oÃ¡n Ä‘iá»u/khoáº£n trong chunk
        refs = extract_refs(sn.get_content(metadata_mode="none"))
        seg = name
        parts = []
        if refs.get("dieu"):
            parts.append(f"Äiá»u {refs['dieu']}")
        if refs.get("khoan"):
            parts.append(f"Khoáº£n {refs['khoan']}")
        if page is not None:
            parts.append(f"Trang {page}")
        if parts:
            seg += " â€“ " + ", ".join(parts)
        cites.append(f"[{seg}]")
    # loáº¡i trÃ¹ng
    uniq = []
    for c in cites:
        if c not in uniq:
            uniq.append(c)
    return " ".join(uniq)


def llm_answer(prompt: str) -> str:
    out = llm_pipe(SYSTEM_PROMPT + "\n\nCÃ¢u há»i: " + prompt + "\n\nTráº£ lá»i:")
    return out[0]["generated_text"].split("Tráº£ lá»i:")[-1].strip()


def answer(question: str, loai: str, nam: str, dieu: str, khoan: str):
    if not question or not question.strip():
        return "HÃ£y nháº­p cÃ¢u há»i.", ""

    # truy váº¥n trÆ°á»›c
    res = query_engine.query(question)

    # kiá»ƒm tra Ä‘á»™ phá»§ & lá»c metadata
    nodes = getattr(res, "source_nodes", [])
    nodes = filter_nodes_by_metadata(nodes, loai or None, int(nam) if nam else None, dieu or None, khoan or None)

    # heuristic: náº¿u khÃ´ng cÃ³ nguá»“n hoáº·c Ä‘iá»ƒm tháº¥p â†’ fallback
    avg_sim = 0.0
    if nodes:
        sims = [getattr(n, "score", 0.0) or 0.0 for n in nodes]
        avg_sim = sum(sims) / max(1, len(sims))

    if not nodes or avg_sim < SIM_THRESHOLD:
        return FALLBACK_MSG, ""

    # Táº¡o cÃ¢u tráº£ lá»i ngáº¯n gá»n dá»±a vÃ o context
    # ghÃ©p ngá»¯ cáº£nh
    context = "\n\n".join([n.get_content(metadata_mode="none") for n in nodes[:3]])
    prompt = (
        SYSTEM_PROMPT
        + "\n\nTÃ i liá»‡u liÃªn quan:\n" + context
        + "\n\nCÃ¢u há»i: " + question
        + "\n\nYÃªu cáº§u: Tráº£ lá»i ngáº¯n gá»n (3â€“6 cÃ¢u), chá»‰ dá»±a vÃ o tÃ i liá»‡u trÃªn, dÃ¹ng tiáº¿ng Viá»‡t, vÃ  chÃ¨n trÃ­ch dáº«n á»Ÿ cuá»‘i."
    )
    text = llm_answer(prompt)

    cites = format_citations(nodes[:TOP_K])
    if cites:
        text = text.rstrip() + "\n\n" + cites
    return text, cites

# =========================
# 5) Upload file & cáº­p nháº­t index
# =========================

def handle_upload(files: List[gr.File]):
    saved = []
    if not files:
        return "ChÆ°a chá»n file."
    for f in files:
        dest = os.path.join(UPLOAD_DIR, os.path.basename(f.name))
        with open(dest, "wb") as w:
            w.write(f.read())
        saved.append(os.path.basename(dest))
    # cáº­p nháº­t index nhanh
    build_or_update_index([DATA_DIR, UPLOAD_DIR])
    global query_engine
    query_engine = index.as_query_engine(similarity_top_k=TOP_K, response_mode="compact")
    return f"ÄÃ£ táº£i lÃªn: {', '.join(saved)} (index Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t)."

# =========================
# 6) Gradio UI
# =========================

def ui_answer(q, loai, nam, dieu, khoan):
    text, _ = answer(q, loai, nam, dieu, khoan)
    return text

with gr.Blocks(title="Chatbot Háº£i quan VN (RAG)") as demo:
    gr.Markdown("""
    # Chatbot Tra cá»©u Nghiá»‡p vá»¥ Háº£i quan (VN)
    - **Ná»™i dung**: Luáº­t / Nghá»‹ Ä‘á»‹nh / ThÃ´ng tÆ° do báº¡n táº£i lÃªn.
    - **NguyÃªn táº¯c**: chá»‰ tráº£ lá»i tá»« tÃ i liá»‡u. Náº¿u thiáº¿u cÄƒn cá»©: *Chá»§ Ä‘á» nÃ y tÃ´i chÆ°a Ä‘Æ°á»£c cáº­p nháº­t. Xin hÃ£y chá»n chá»§ Ä‘á» khÃ¡c.*
    """)

    with gr.Row():
        q = gr.Textbox(label="CÃ¢u há»i", placeholder="VÃ­ dá»¥: Äiá»u kiá»‡n miá»…n kiá»ƒm tra thá»±c táº¿ hÃ ng hÃ³a theo ThÃ´ng tÆ° 39/2018?", scale=3)
    with gr.Row():
        loai = gr.Dropdown(label="Loáº¡i vÄƒn báº£n", choices=["", "Luáº­t", "Nghá»‹ Ä‘á»‹nh", "ThÃ´ng tÆ°"], value="")
        nam = gr.Dropdown(label="NÄƒm", choices=[""] + [str(y) for y in range(2005, 2031)], value="")
        dieu = gr.Textbox(label="Äiá»u (vÃ­ dá»¥ 10)", placeholder="", scale=1)
        khoan = gr.Textbox(label="Khoáº£n (vÃ­ dá»¥ 2)", placeholder="", scale=1)

    btn = gr.Button("Há»i")
    out = gr.Markdown()

    btn.click(fn=ui_answer, inputs=[q, loai, nam, dieu, khoan], outputs=[out])

    gr.Markdown("""
    ### Táº£i thÃªm vÄƒn báº£n (PDF/DOCX/TXT)
    Chá»n file Ä‘á»ƒ thÃªm vÃ o cÆ¡ sá»Ÿ tri thá»©c. Há»‡ thá»‘ng sáº½ **re-index** tá»± Ä‘á»™ng.
    """)
    uploader = gr.File(label="Táº£i tá»‡p (nhiá»u tá»‡p)", file_types=[".pdf", ".docx", ".txt"], file_count="multiple")
    upmsg = gr.Markdown()
    uploader.upload(fn=handle_upload, inputs=[uploader], outputs=[upmsg])

    gr.Markdown(f"Cáº­p nháº­t láº§n cuá»‘i: {time.strftime('%Y-%m-%d %H:%M:%S')} (giá» mÃ¡y chá»§)")

if __name__ == "__main__":
    demo.launch()
```

---

## 5) Kiá»ƒm thá»­ nhanh (UAT)

* **Bá»™ 10 cÃ¢u** (Ä‘iá»u chá»‰nh theo bá»™ tÃ i liá»‡u cá»§a báº¡n):

  1. *Äiá»u kiá»‡n miá»…n kiá»ƒm tra thá»±c táº¿ hÃ ng hÃ³a theo ThÃ´ng tÆ° 39/2018 lÃ  gÃ¬?*
  2. *ThÃ nh pháº§n há»“ sÆ¡ khi Ä‘Äƒng kÃ½ tá» khai háº£i quan Ä‘iá»‡n tá»­ theo Nghá»‹ Ä‘á»‹nh X?*
  3. *CÃ¡ch xÃ¡c Ä‘á»‹nh trá»‹ giÃ¡ tÃ­nh thuáº¿ Ä‘á»‘i vá»›i hÃ ng nháº­p kháº©u theo â€¦?*
  4. *Thá»i háº¡n ná»™p thuáº¿ xuáº¥t kháº©u Ä‘á»‘i vá»›i â€¦?*
  5. *Quy Ä‘á»‹nh vá» miá»…n, giáº£m, hoÃ n thuáº¿ trong trÆ°á»ng há»£p â€¦?*
  6. *TrÃ¡ch nhiá»‡m cá»§a ngÆ°á»i khai háº£i quan theo Luáº­t â€¦?*
  7. *Quy Ä‘á»‹nh vá» kiá»ƒm tra sau thÃ´ng quan â€¦?*
  8. *Quy Ä‘á»‹nh xá»­ pháº¡t vi pháº¡m hÃ nh chÃ­nh vá»›i hÃ nh vi â€¦?*
  9. *CÄƒn cá»© phÃ¡p lÃ½ vá» xuáº¥t xá»© hÃ ng hÃ³a â€¦?*
  10. *Thá»§ tá»¥c vá»›i hÃ ng quÃ¡ cáº£nh theo â€¦?*

**TiÃªu chÃ­ Ä‘áº¡t**

* â‰¥90% cÃ¢u cÃ³ Ã­t nháº¥t **1 trÃ­ch dáº«n** Ä‘Ãºng vÄƒn báº£n vÃ  (náº¿u cÃ³) Ä‘Ãºng **Äiá»u/Khoáº£n**.
* Khi thiáº¿u cÄƒn cá»© â†’ tráº£ vá» **thÃ´ng Ä‘iá»‡p báº¯t buá»™c**.
* Thá»i gian pháº£n há»“i trÃªn CPU miá»…n phÃ­: < 20s vá»›i cÃ¢u há»i ngáº¯n.

---

## 6) Gá»£i Ã½ váº­n hÃ nh & cáº­p nháº­t

* Äáº·t tÃªn file rÃµ rÃ ng, vÃ­ dá»¥: `Thong-tu-39-2018-BTC.pdf`, `Nghi-dinh-xx-2024.pdf`.
* Khi cÃ³ vÄƒn báº£n má»›i/sá»­a Ä‘á»•i: chá»‰ cáº§n **táº£i lÃªn** qua UI hoáº·c thÃªm vÃ o `data/` rá»“i push/rebuild.
* Vá»›i dá»¯ liá»‡u tÄƒng dáº§n: cÃ¢n nháº¯c chuyá»ƒn embeddings sang `bge-m3` hoáº·c `intfloat/multilingual-e5-large`.

---

## 7) CÃ¢u há»i thÆ°á»ng gáº·p (FAQ)

**Há»i:** Sao cÃ¢u tráº£ lá»i Ä‘Ã´i khi chung chung?
**ÄÃ¡p:** Do dÃ¹ng model nhá» Ä‘á»ƒ cháº¡y miá»…n phÃ­ CPU. VÃ¬ lÃ  RAG, cháº¥t lÆ°á»£ng phá»¥ thuá»™c Ä‘á»™ sÃ¡t cá»§a Ä‘oáº¡n trÃ­ch. HÃ£y tÄƒng cháº¥t lÆ°á»£ng tÃ i liá»‡u, Ä‘áº·t cÃ¢u há»i cá»¥ thá»ƒ, hoáº·c nÃ¢ng cáº¥p model.

**Há»i:** CÃ³ thá»ƒ báº¯t buá»™c tráº£ lá»i chá»‰ tá»« tÃ i liá»‡u khÃ´ng?
**ÄÃ¡p:** CÃ³. Prompt há»‡ thá»‘ng + ngÆ°á»¡ng `SIM_THRESHOLD` + fallback á»Ÿ trÃªn Ä‘Ã£ giá»›i háº¡n. CÃ³ thá»ƒ háº¡ `TOP_K`/tÄƒng `SIM_THRESHOLD` Ä‘á»ƒ nghiÃªm hÆ¡n.

**Há»i:** CÃ³ thá»ƒ xuáº¥t cÃ¢u tráº£ lá»i ra PDF?
**ÄÃ¡p:** CÃ³ thá»ƒ bá»• sung má»™t nÃºt táº¡o PDF (reportlab) â€” pháº§n má»Ÿ rá»™ng nhá», khÃ´ng báº¯t buá»™c.

---

> **Báº£n quyá»n & trÃ¡ch nhiá»‡m**: Máº«u nÃ y phá»¥c vá»¥ má»¥c Ä‘Ã­ch thÃ´ng tin. Vui lÃ²ng Ä‘á»‘i chiáº¿u vÄƒn báº£n phÃ¡p luáº­t chÃ­nh thá»©c khi sá»­ dá»¥ng trong tÃ¬nh huá»‘ng phÃ¡p lÃ½ cá»¥ thá»ƒ.
